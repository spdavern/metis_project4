{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivalence of normalize and unit_norm\n",
    "\n",
    "I created a function to 'normalize' my doc_topic matrix points using the function\n",
    "$\\left(\\frac{x}{\\sqrt{(x^2+y^2)}},\\frac{y}{\\sqrt{(x^2+y^2)}}\\right)$ based on a suggestion from Roberto.  I later found out about sklearn.preprocessing.normalize.  This notebook is aimed at determining if they give the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:02:43.294328Z",
     "start_time": "2019-11-11T23:02:41.939552Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from src.functions import unit_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm including this next cell only to get the vector I was normalizing when I encountered this issue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:02:51.968690Z",
     "start_time": "2019-11-11T23:02:43.297124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 topics variance ratios: [0.02570641 0.0142085 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.25876704, -0.39405516],\n",
       "       [ 1.04318722, -0.5211399 ],\n",
       "       [ 0.69086968, -0.71191713],\n",
       "       ...,\n",
       "       [ 0.65087842, -0.67896504],\n",
       "       [ 1.04650948, -0.71010517],\n",
       "       [ 0.78235133, -0.67294219]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from src.functions import get_sents\n",
    "# Read in the transcripts\n",
    "raw_data_path = './data/raw/ted-talks/'\n",
    "transcripts_filename = 'transcripts.csv'\n",
    "t_df = pd.read_csv(raw_data_path+transcripts_filename)\n",
    "# Pare down the corpus to only those talks with the word 'love'\n",
    "love=t_df[t_df['transcript'].str.contains('love',case=False)]\n",
    "# Tokenize\n",
    "# Get the collection of n(=5)-sentence snippets with the word 'love'\n",
    "love_snippets = get_sents(love,'love',0,0)\n",
    "# Topic modeling\n",
    "# Vectorize\n",
    "cv1 = CountVectorizer(stop_words='english',binary=True)\n",
    "cv_doc_word = cv1.fit_transform(love_snippets.love)\n",
    "# Dimension Reduction\n",
    "cv_lsa=[]\n",
    "cv_doc_topic=[]\n",
    "for i in range(2,3):\n",
    "    cv_lsa.append(TruncatedSVD(i))\n",
    "    cv_doc_topic.append(cv_lsa[i-2].fit_transform(cv_doc_word))\n",
    "    print(int(i),'topics variance ratios:',cv_lsa[i-2].explained_variance_ratio_)\n",
    "cv_doc_topic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will normalize using two functions: sklearn.preprocessing.normalize and src.unit_norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:02:52.471162Z",
     "start_time": "2019-11-11T23:02:51.971604Z"
    }
   },
   "outputs": [],
   "source": [
    "x=normalize(cv_doc_topic[0])\n",
    "y=unit_norm(pd.DataFrame(cv_doc_topic[0]),demean=False).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:02:52.476978Z",
     "start_time": "2019-11-11T23:02:52.473113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x==y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:02:52.485564Z",
     "start_time": "2019-11-11T23:02:52.479463Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    if x[i][0]!=y[i][0] or x[i][1]!=y[i][1]:\n",
    "        print(i, x[i][0]-y[i][0],x[i][1]-y[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows that the values were essentially the same but not necessarily to the last decimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
